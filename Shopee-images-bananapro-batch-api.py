import os
import json
import time
import base64
import mimetypes

import requests
import pandas as pd
from google import genai
from google.genai import types

# ===== æŠŠä½ çš„ API Key æ”¾åœ¨é€™è£¡ =====
API_KEY = "æŠŠä½ çš„ API Key æ”¾åœ¨é€™è£¡"

# Nano Banana Proï¼ˆGemini 3 Pro Image é è¦½ï¼‰
IMAGE_MODEL = "gemini-3-pro-image-preview"

# ===== æª”æ¡ˆè¨­å®š =====
PRODUCT_FILE = "products.xlsx"                     # è¼¸å…¥ï¼šSKU / å•†å“åç¨± / å•†å“æ•˜è¿° / å•†å“åœ–URL
JSONL_FILE = "image_batch_with_base.jsonl"         # çµ¦ Batch API ç”¨çš„è«‹æ±‚æª”
TMP_IMG_DIR = "tmp_base_images"                    # æš«å­˜åŸå§‹å•†å“åœ–
OUTPUT_DIR = "output_images_batch"                 # ç”¢å‡ºçš„ä¸»åœ–
SKIPPED_FILE = "batch_skipped_products.xlsx"       # å¤±æ•—æ¸…å–®

# å»ºç«‹ Gemini Client
client = genai.Client(api_key=API_KEY)


def safe_str(v) -> str:
    """æŠŠ None / NaN è®Šæˆç©ºå­—ä¸²ï¼Œé †ä¾¿ stripã€‚"""
    if v is None:
        return ""
    s = str(v)
    if s.lower() == "nan":
        return ""
    return s.strip()


def guess_mime_and_ext(url: str, resp: requests.Response):
    """å¾ Content-Type æˆ– URL çŒœåœ–ç‰‡æ ¼å¼ã€‚"""
    ct = resp.headers.get("Content-Type", "").lower()
    if "png" in ct:
        return "image/png", ".png"
    if "webp" in ct:
        return "image/webp", ".webp"
    if "jpeg" in ct or "jpg" in ct:
        return "image/jpeg", ".jpg"

    mt, _ = mimetypes.guess_type(url)
    if mt and mt.startswith("image/"):
        ext = mimetypes.guess_extension(mt) or ".jpg"
        return mt, ext

    # æœ€ä¿å®ˆï¼šç•¶ä½œ jpeg
    return "image/jpeg", ".jpg"


def download_image(url: str, sku: str):
    """å¾ URL ä¸‹è¼‰åœ–ç‰‡ï¼Œå›å‚³ (image_path, mime_type)ï¼Œå¤±æ•—å›å‚³ Noneã€‚"""
    if not url:
        print(f"âš ï¸ SKU={sku} æ²’æœ‰åœ–ç‰‡ URLï¼Œç•¥éä¸‹è¼‰ã€‚")
        return None

    os.makedirs(TMP_IMG_DIR, exist_ok=True)

    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
    except Exception as e:
        print(f"âš ï¸ SKU={sku} ä¸‹è¼‰åœ–ç‰‡å¤±æ•—ï¼š{e}")
        return None

    mime_type, ext = guess_mime_and_ext(url, resp)
    filename_safe = sku.replace("/", "_").replace("\\", "_")
    img_path = os.path.join(TMP_IMG_DIR, f"{filename_safe}{ext}")
    with open(img_path, "wb") as f:
        f.write(resp.content)

    return img_path, mime_type


def build_image_prompt(name: str, desc: str) -> str:
    """
    çµ¦ Nano Banana Pro çš„æŒ‡ä»¤ï¼š
    - ä¸€å®šè¦ç”¨ã€Œæä¾›çš„å•†å“åŸå§‹ç…§ç‰‡ã€ç•¶ä¸»é«”ï¼Œä¸é‡ç•«ç”¢å“
    - è‡ªå·±å¾å•†å“æ•˜è¿°ä¸­æŒ‘ 2~3 å€‹è³£é»ï¼Œåšæ¨™é¡Œ / å‰¯æ¨™é¡Œ / icon è³£é»
    """
    name = safe_str(name)
    desc = safe_str(desc)
    short_desc = desc[:400]  # é¿å… prompt å¤ªé•·

    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆé–€ç‚ºå°ç£è¦çš®è³£å®¶è¨­è¨ˆ 1:1 ä¸»åœ–çš„é›»å•†è¦–è¦ºè¨­è¨ˆå¸«ã€‚

ç³»çµ±æœƒæä¾›ä½ ä¸€å¼µå•†å“åŸå§‹ç…§ç‰‡ï¼Œè«‹ä»¥é‚£å¼µç…§ç‰‡ç‚ºä¸»é«”ï¼Œ
å¹«æˆ‘è¨­è¨ˆä¸€å¼µ 1:1 æ¯”ä¾‹ã€è¦–è¦ºå¸å¼•åŠ›å¼·ã€æ–‡å­—æ¥µç°¡çš„é›»å•†ä¸»åœ–ã€‚

ã€æ•´é«”ç‰ˆå‹ã€‘
- ä¸Šæ–¹ï¼šä¸€è¡Œä¸»æ¨™ï¼Œå¿…è¦æ™‚å†åŠ ä¸€è¡Œå¾ˆçŸ­çš„å‰¯æ¨™ã€‚
- å·¦å´ä¸­é–“(ä¸æ“‹åˆ°å­—åŠå•†å“å³å¯)ï¼šå‚ç›´æ’åˆ— 2ï½3 å€‹ã€Œicon + å¾ˆçŸ­æ–‡å­—æˆ–ç›´æ¥çœç•¥æ–‡å­—ã€çš„è³£é»è† å›Šã€‚
- ä¸­å¤®åä¸‹ï¼šå¤§é¢ç©é¡¯ç¤ºå•†å“æœ¬é«”ï¼Œæ­é…ç°¡æ½”å ´æ™¯èƒŒæ™¯ã€‚
- å³å´ï¼šå¦‚æœå•†å“æœ‰éœ€è¦å¼·èª¿çš„ç´°ç¯€ï¼Œä¸€å€‹å°çš„æ”¾å¤§åœˆæˆ–æ°£æ³¡ï¼Œå¼·èª¿å•†å“æŸå€‹é—œéµç‰¹å¾µï¼Œå¯ä»¥æ­é…ä¸€å€‹è¶…çŸ­æ–‡å­—ï¼Œæˆ–ç›´æ¥çœç•¥ã€‚
- å³ä¸‹è§’ï¼šå¯ä»¥æœ‰ä¸€å€‹å°æ¨™ç±¤ï¼Œä½†æ–‡å­—ä¹Ÿè¦éå¸¸çŸ­ï¼Œæˆ–ç›´æ¥çœç•¥ã€‚

ã€å•†å“ç…§ç‰‡ä½¿ç”¨è¦å‰‡ã€‘
- ä¸€å®šè¦ä½¿ç”¨æˆ‘æä¾›çš„å•†å“åœ–ç‰‡ä½œç‚ºä¸»è§’ã€‚
- ä¸è¦æ”¹è®Šå•†å“æœ¬é«”çš„å¤–è§€ã€å½¢ç‹€èˆ‡é¡è‰²ï¼Œä¸è¦æŠŠå•†å“æ›æˆåˆ¥çš„æ±è¥¿ã€‚
- å¯ä»¥èª¿æ•´èƒŒæ™¯ã€å…‰ç·šã€æ§‹åœ–èˆ‡åŠ ä¸Šæ–‡å­—ã€åœ–æ¨™ï¼Œä½†ä¸è¦è®“ç•«é¢è®Šå¾—å¤ªèŠ±ã€‚
- è«‹æŠŠå•†å“æ”¾åœ¨ç•«é¢ä¸­å¤®æˆ–ç•¥å¾®åä¸‹çš„ä½ç½®ï¼Œä¿æŒæ¸…æ¥šã€ç«‹é«”ã€æœ‰è³ªæ„Ÿã€‚

ã€èƒŒæ™¯èˆ‡å ´æ™¯ã€‘
- èƒŒæ™¯è«‹è¨­è¨ˆç‚ºç°¡æ½”ä½†æœ‰å±¤æ¬¡çš„å ´æ™¯æˆ–æ¼¸å±¤è‰²ï¼Œé¡è‰²èˆ‡å•†å“æœ¬èº«å”èª¿ã€‚
- å¯ä»¥åŠ å…¥èˆ‡å•†å“ç”¨é€”ç›¸é—œçš„æ¨¡ç³Šå ´æ™¯å…ƒç´ ï¼Œä½†è¦ä¿æŒç°¡å–®ï¼Œä¸è¦å¤ªå¤šç´°ç¯€ã€‚
- ç›®æ¨™æ˜¯è®“å•†å“å’Œä¸»æ¨™æœ€é†’ç›®ï¼Œè€Œä¸æ˜¯èƒŒæ™¯æˆ–ç‰¹æ•ˆã€‚

ã€æ–‡æ¡ˆç”¢ç”Ÿè¦å‰‡ï¼ˆé‡é»ï¼šå­—è¦å°‘ï¼ï¼‰ã€‘
è«‹ä½ æ ¹æ“šå•†å“åç¨±èˆ‡å•†å“æ•˜è¿°ï¼Œè‡ªè¡ŒæŒ‘é¸ä¸¦æ’°å¯«ä»¥ä¸‹æ–‡å­—ï¼Œå‹™å¿…æ§åˆ¶å­—æ•¸ï¼š

1. ä¸»æ¨™ï¼ˆé†’ç›®æ¨™é¡Œï¼‰
   - ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
   - å­—æ•¸é™åˆ¶ï¼š4ï½8 å€‹ä¸­æ–‡å­—ã€‚
   - èªæ°£ç°¡æ½”æœ‰åŠ›ï¼Œèƒ½å¿«é€Ÿèªªæ˜é€™å€‹å•†å“ã€Œæœ€é‡è¦çš„æ ¸å¿ƒåƒ¹å€¼ã€æˆ–ã€Œä¸»è¦ç”¨é€”ã€ã€‚
   - ä¸è¦å¯«æˆå¥å­ï¼Œä¸è¦æœ‰æ¨™é»ï¼Œåªè¦çŸ­èªï¼Œä¾‹å¦‚ï¼šã€Œç©©å®šæ°´è³ªé˜²ç•°å‘³ã€ã€ã€Œè»Šé€Ÿå³æ™‚é¡¯ç¤ºã€ã€‚

2. å‰¯æ¨™ï¼ˆå¯é¸ï¼‰
   - ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
   - å­—æ•¸é™åˆ¶ï¼šæœ€å¤š 10 å€‹ä¸­æ–‡å­—ã€‚
   - åªæœ‰åœ¨çœŸçš„æœ‰å¿…è¦è£œå……æ™‚æ‰åŠ ä¸Šä¸€è¡Œå‰¯æ¨™ï¼Œå¦å‰‡å¯ä»¥å®Œå…¨ä¸æ”¾å‰¯æ¨™ã€‚
   - å¦‚æœç„¡æ³•åœ¨ 10 å€‹å­—å…§æ¸…æ¥šè¡¨é”ï¼Œå°±ä¹¾è„†ä¸è¦æ”¾å‰¯æ¨™ã€‚

3. é—œéµç‰¹æ€§è³£é»ï¼ˆæ­é… iconï¼‰
   - è«‹æ•´ç†å‡º 2ï½3 å€‹æœ€é‡è¦çš„åŠŸèƒ½æˆ–å„ªé»ã€‚
   - æ¯å€‹è³£é»æ–‡å­—é™åˆ¶ï¼š3ï½5 å€‹ä¸­æ–‡å­—ã€‚
   - é€™äº›æ–‡å­—æœƒå‡ºç¾åœ¨å·¦å´ icon è† å›Šæˆ–å³å´æ”¾å¤§åœˆé™„è¿‘ã€‚
   - ä¸è¦å¯«æˆå¥å­ï¼Œåªç”¨çŸ­èªï¼Œä¾‹å¦‚ï¼šã€Œæ·¡æµ·æ°´ç”¨ã€ã€ã€Œå¿«é€Ÿå®šä½ã€ã€ã€Œç¯€çœç©ºé–“ã€ã€‚

4. æ–‡æ¡ˆä¾†æºé™åˆ¶
   - æ‰€æœ‰æ–‡å­—å…§å®¹å¿…é ˆæœ‰æ ¹æ“šï¼Œåªèƒ½ä¾†è‡ªä¸‹æ–¹å•†å“æ•˜è¿°æˆ–å…¶åˆç†æ¦‚æ‹¬èˆ‡ç¸®å¯«ã€‚
   - ä¸å¯ä»¥æ†‘ç©ºæ–°å¢å•†å“æ²’æœ‰çš„åŠŸèƒ½æˆ–èª‡å¤§ç™‚æ•ˆã€‚
   - å¦‚æœéœ€è¦ç¸®çŸ­ï¼Œè«‹å„ªå…ˆåˆªæ‰ä¸é‡è¦çš„å­—ï¼Œè€Œä¸æ˜¯åŠ æ–°è³‡è¨Šã€‚

5. æ–‡å­—ç¸½é‡é™åˆ¶ï¼ˆéå¸¸é‡è¦ï¼‰
   - æ•´å¼µåœ–ä¸Šæ‰€æœ‰ä¸­æ–‡å­—ç¸½æ•¸ï¼Œå»ºè­°æ§åˆ¶åœ¨ã€Œä¸»æ¨™ + ï¼ˆå¯é¸ï¼‰ä¸€è¡Œå‰¯æ¨™ + æœ€å¤š 3 å€‹çŸ­è³£é»ã€çš„ç¯„åœå…§ã€‚
   - è«‹ä¸è¦å†é¡å¤–åŠ å…¥å…¶ä»–æ®µè½æ–‡å­—ã€èªªæ˜å¥ã€è¦æ ¼é•·å¥æˆ–å“ç‰Œæ•…äº‹ã€‚
   - ç›®æ¨™æ˜¯ã€Œç•«é¢ä¹¾æ·¨ã€æ–‡å­—æ¥µç°¡ã€ï¼Œè®“äººä¸€çœ¼å°±æ‡‚ï¼Œä¸éœ€è¦é–±è®€å¾ˆå¤šå­—ã€‚

ã€å•†å“åŸºæœ¬è³‡è¨Šã€‘
- å•†å“åç¨±ï¼ˆåƒ…ä¾›ä½ ç†è§£ï¼Œä¸ä¸€å®šè¦å®Œæ•´å¯«åœ¨ä¸»æ¨™è£¡ï¼‰ï¼š{name}

ã€å•†å“æ•˜è¿°ï¼ˆè«‹ä»¥é€™æ®µå…§å®¹ç‚ºä¾æ“šï¼Œè‡ªå·±æŒ‘é¸ã€ç¸®çŸ­åˆé©çš„æ–‡æ¡ˆï¼‰ã€‘
{short_desc}

è«‹ç›´æ¥åœ¨ç”Ÿæˆçš„åœ–ç‰‡ä¸­å‘ˆç¾ä½ è¨­è¨ˆå¥½çš„ä¸»æ¨™ã€å‰¯æ¨™èˆ‡è³£é»æ–‡å­—ï¼Œ
ä¸”å‹™å¿…éµå®ˆã€Œæ¯æ®µæ–‡å­—éƒ½å¾ˆçŸ­ã€æ•´é«”æ–‡å­—ç¸½é‡å¾ˆå°‘ã€çš„è¦æ±‚ã€‚
è«‹ç›´æ¥è¼¸å‡ºå®Œæˆè¨­è¨ˆå¾Œçš„åœ–ç‰‡ï¼Œä¸è¦é¡å¤–è¼¸å‡ºä»»ä½•èªªæ˜æ–‡å­—ã€‚
"""
    return prompt


def build_jsonl_and_product_map(products_path: str, jsonl_path: str):
    """
    è®€ products.xlsxï¼š

      - æœ‰åœ–ç‰‡ URL ä¸”ä¸‹è¼‰æˆåŠŸ â†’ ä¸Šå‚³ Files APIï¼Œå¯«å…¥å¸¶ fileData çš„ Batch è«‹æ±‚
      - æ²’æœ‰åœ–ç‰‡ URL â†’ ç•¥éï¼Œä¸é€é€² Batchï¼Œè¨˜éŒ„åˆ° skipped æ¸…å–®ï¼ˆå¤±æ•—åŸå› ï¼šç„¡åœ–ç‰‡URLï¼‰
      - åœ–ç‰‡ä¸‹è¼‰å¤±æ•— â†’ ç•¥éï¼Œä¸é€é€² Batchï¼Œè¨˜éŒ„åˆ° skipped æ¸…å–®ï¼ˆå¤±æ•—åŸå› ï¼šåœ–ç‰‡ä¸‹è¼‰å¤±æ•—ï¼‰

    å›å‚³ï¼š
      product_row_map: {SKU -> åŸå§‹é‚£ä¸€åˆ—çš„ dict}
      pre_skipped_rows:  å‰ç½®éšæ®µå°±è¢«ç•¥éçš„åˆ—ï¼ˆå·²å«ã€Œå¤±æ•—åŸå› ã€ï¼‰
      base_columns:      products.xlsx çš„æ¬„ä½é †åºï¼Œç”¨ä¾†ä¹‹å¾Œçµ„æ¸…å–®
    """
    print(f"è®€å– Excelï¼š{products_path}")
    df = pd.read_excel(products_path)

    required_cols = ["SKU", "å•†å“åç¨±", "å•†å“æ•˜è¿°", "å•†å“åœ–URL"]
    for col in required_cols:
        if col not in df.columns:
            raise KeyError(f"åœ¨ {products_path} è£¡æ‰¾ä¸åˆ°æ¬„ä½ï¼š{col}")

    base_columns = list(df.columns)
    product_row_map = {}      # æ‰€æœ‰ SKU å°æ‡‰åŸå§‹åˆ—ï¼ˆä¹‹å¾Œ Batch è§£æç”¨ï¼‰
    pre_skipped_rows = []     # é‚„æ²’é€² Batch å‰å°±è¢«ç•¥éçš„ï¼ˆæ²’åœ– / ä¸‹è¼‰å¤±æ•—ï¼‰

    with open(jsonl_path, "w", encoding="utf-8") as f:
        for idx, row in df.iterrows():
            sku = safe_str(row.get("SKU")) or f"row_{idx:04d}"
            name = safe_str(row.get("å•†å“åç¨±"))
            desc = safe_str(row.get("å•†å“æ•˜è¿°"))
            img_url = safe_str(row.get("å•†å“åœ–URL"))

            row_dict = row.to_dict()
            product_row_map[sku] = row_dict

            # âŒ æƒ…æ³ 1ï¼šå®Œå…¨æ²’æœ‰åœ–ç‰‡ URL â†’ ç›´æ¥ç•¥éï¼Œè¨˜éŒ„
            if not img_url:
                print(f"âš ï¸ ç¬¬ {idx} åˆ—ï¼ˆSKU={sku}ï¼‰æ²’æœ‰åœ–ç‰‡ URLï¼Œæ­¤å•†å“ä¸é€é€² Batchï¼ˆç•¥éï¼‰ã€‚")
                row_failed = dict(row_dict)
                row_failed["å¤±æ•—åŸå› "] = "ç„¡åœ–ç‰‡URL"
                pre_skipped_rows.append(row_failed)
                continue

            # æœ‰åœ–ç‰‡ URL â†’ å˜—è©¦ä¸‹è¼‰
            img_info = download_image(img_url, sku)
            if not img_info:
                # âŒ æƒ…æ³ 2ï¼šåœ–ç‰‡ URL æœ‰ï¼Œä½†ä¸‹è¼‰å¤±æ•— â†’ ä¹Ÿç•¥éï¼Œè¨˜éŒ„
                print(f"âš ï¸ SKU={sku} åœ–ç‰‡ä¸‹è¼‰å¤±æ•—ï¼Œæ­¤å•†å“ä¸é€é€² Batchï¼ˆç•¥éï¼‰ã€‚")
                row_failed = dict(row_dict)
                row_failed["å¤±æ•—åŸå› "] = "åœ–ç‰‡ä¸‹è¼‰å¤±æ•—"
                pre_skipped_rows.append(row_failed)
                continue

            # âœ… åªæœ‰ã€Œæœ‰ URL ä¸”ä¸‹è¼‰æˆåŠŸã€æ‰æœƒèµ°åˆ°é€™è£¡
            img_path, mime_type = img_info
            print(f"SKU={sku} åœ–ç‰‡ä¸‹è¼‰å®Œæˆï¼š{img_path}")

            # ä¸Šå‚³åœ–ç‰‡åˆ° Files API
            uploaded_file = client.files.upload(
                file=img_path,
                config=types.UploadFileConfig(
                    display_name=f"sku-{sku}",
                    mime_type=mime_type,
                ),
            )

            # ç­‰å¾…æª”æ¡ˆè™•ç†å®Œæˆï¼ˆé€šå¸¸å¾ˆå¿«ï¼‰
            while getattr(uploaded_file, "state", None) and getattr(
                uploaded_file.state, "name", ""
            ) == "PROCESSING":
                time.sleep(1)
                uploaded_file = client.files.get(name=uploaded_file.name)

            file_uri = getattr(uploaded_file, "uri", None) or uploaded_file.name

            prompt = build_image_prompt(name, desc)

            # ä¸€è¡Œ Batch JSONL è«‹æ±‚ï¼šå¸¶ fileData + prompt
            req = {
                "key": sku,
                "request": {
                    "contents": [{
                        "role": "user",
                        "parts": [
                            {
                                "fileData": {
                                    "fileUri": file_uri,
                                    "mimeType": mime_type,
                                }
                            },
                            {"text": prompt},
                        ],
                    }],
                    "generation_config": {"responseModalities": ["IMAGE"]},
                },
            }

            f.write(json.dumps(req, ensure_ascii=False) + "\n")

    print(f"âœ” å·²ç”¢ç”Ÿ Batch è«‹æ±‚æª”ï¼š{jsonl_path}")

    return product_row_map, pre_skipped_rows, base_columns


def run_batch_and_save_images(
    jsonl_path: str,
    output_dir: str,
    product_row_map: dict,
    pre_skipped_rows: list[dict],
    base_columns: list[str],
):
    """å‘¼å« Batch APIï¼Œç­‰å®ƒè·‘å®Œï¼ŒæŠŠåœ–ç‰‡å­˜èµ·ä¾†ï¼Œæ‰€æœ‰å¤±æ•—/ç•¥éçš„çµ±ä¸€å¯«åˆ° SKIPPED_FILEã€‚"""
    os.makedirs(output_dir, exist_ok=True)

    # 1) ä¸Šå‚³ JSONL åˆ° Files API
    uploaded_file = client.files.upload(
        file=jsonl_path,
        config=types.UploadFileConfig(
            display_name="shopee-image-batch-with-base",
            mime_type="jsonl",
        ),
    )
    print(f"âœ” å·²ä¸Šå‚³ JSONL æª”ï¼š{uploaded_file.name}")

    # 2) å»ºç«‹ Batch Job
    batch_job = client.batches.create(
        model=IMAGE_MODEL,
        src=uploaded_file.name,
        config={"display_name": "shopee-image-batch-with-base"},
    )
    job_name = batch_job.name
    print(f"âœ” å»ºç«‹ Batch Jobï¼š{job_name}")

    # 3) ç­‰å¾… Batch å®Œæˆ
    done_states = {
        "JOB_STATE_SUCCEEDED",
        "JOB_STATE_FAILED",
        "JOB_STATE_CANCELLED",
        "JOB_STATE_EXPIRED",
    }

    print("â³ é–‹å§‹è¼ªè©¢ Batch ç‹€æ…‹...")
    job = client.batches.get(name=job_name)
    while job.state.name not in done_states:
        print(f"ç›®å‰ç‹€æ…‹ï¼š{job.state.name}")
        time.sleep(30)  # å¯è‡ªè¡Œèª¿æ•´è¼ªè©¢é–“éš”
        job = client.batches.get(name=job_name)

    print(f"âœ… Batch çµæŸï¼Œç‹€æ…‹ï¼š{job.state.name}")

    if job.state.name != "JOB_STATE_SUCCEEDED":
        if job.error:
            print("Batch å¤±æ•—åŸå› ï¼š", job.error)
        # å³ä½¿æ•´å€‹ Batch å¤±æ•—ï¼Œä¹ŸæŠŠ pre_skipped_rows å­˜èµ·ä¾†
        all_skipped_rows = list(pre_skipped_rows)
        if all_skipped_rows:
            df_skip = pd.DataFrame(all_skipped_rows)
            # ç¢ºä¿æ¬„ä½é †åºï¼šåŸæœ¬æ¬„ä½ + å¤±æ•—åŸå› 
            cols = base_columns + ["å¤±æ•—åŸå› "]
            for col in cols:
                if col not in df_skip.columns:
                    df_skip[col] = None
            df_skip = df_skip[cols]
            df_skip.to_excel(SKIPPED_FILE, index=False)
            print(f"âš ï¸ å…± {len(all_skipped_rows)} ç­†å•†å“å¤±æ•—/ç•¥éï¼Œå·²è¼¸å‡ºåˆ°ï¼š{SKIPPED_FILE}")
        return

    # 4) ä¸‹è¼‰çµæœ JSONL
    result_file_name = job.dest.file_name
    file_bytes = client.files.download(file=result_file_name)
    content = file_bytes.decode("utf-8")

    # â­ æŠŠå‰ç½®éšæ®µå¤±æ•—çš„å…ˆä¸Ÿé€²ä¾†
    all_skipped_rows: list[dict] = list(pre_skipped_rows)

    for line in content.splitlines():
        if not line.strip():
            continue

        obj = json.loads(line)
        key = obj.get("key", "no_key")
        resp = obj.get("response")

        # å°å·¥å…·ï¼šå¾ product_row_map æ‹¿åŸå§‹åˆ—ï¼Œå¡ä¸Šå¤±æ•—åŸå› 
        def add_skip(reason: str):
            base_row = product_row_map.get(key)
            if base_row is None:
                row_dict = {col: None for col in base_columns}
                if "SKU" in base_columns:
                    row_dict["SKU"] = key
            else:
                row_dict = dict(base_row)
            row_dict["å¤±æ•—åŸå› "] = reason
            all_skipped_rows.append(row_dict)

        if not resp:
            print(f"[{key}] æ²’æœ‰ responseï¼Œå¯èƒ½è©²ç­†å¤±æ•—ï¼š{obj.get('status') or obj}")
            add_skip("Batchç„¡å›æ‡‰")
            continue

        try:
            parts = resp["candidates"][0]["content"]["parts"]
        except (KeyError, IndexError, TypeError):
            print(f"[{key}] response çµæ§‹ä¸å¦‚é æœŸï¼š{resp}")
            add_skip("Batchå›æ‡‰çµæ§‹éŒ¯èª¤")
            continue

        saved = False
        for part in parts:
            inline = part.get("inlineData") or part.get("inline_data")
            if not inline:
                continue
            data_b64 = inline.get("data")
            if not data_b64:
                continue

            img_bytes = base64.b64decode(data_b64)
            filename_safe = key.replace("/", "_").replace("\\", "_")
            out_path = os.path.join(output_dir, f"{filename_safe}.png")
            with open(out_path, "wb") as img_f:
                img_f.write(img_bytes)

            print(f"[{key}] åœ–ç‰‡å·²å„²å­˜ï¼š{out_path}")
            saved = True
            break

        if not saved:
            print(f"[{key}] å›æ‡‰ä¸­æ²’æœ‰åœ–ç‰‡è³‡æ–™ã€‚")
            add_skip("Batchå›æ‡‰ç„¡åœ–ç‰‡")

    # 5) æŠŠã€Œæ‰€æœ‰å¤±æ•—/ç•¥éã€çµ±ä¸€å¯«æˆä¸€ä»½ Excel
    if all_skipped_rows:
        df_skip = pd.DataFrame(all_skipped_rows)
        cols = base_columns + ["å¤±æ•—åŸå› "]
        for col in cols:
            if col not in df_skip.columns:
                df_skip[col] = None
        df_skip = df_skip[cols]
        df_skip.to_excel(SKIPPED_FILE, index=False)
        print(f"âš ï¸ å…± {len(all_skipped_rows)} ç­†å•†å“å¤±æ•—/ç•¥éï¼Œå·²è¼¸å‡ºåˆ°ï¼š{SKIPPED_FILE}")
    else:
        print("ğŸ‰ æ‰€æœ‰é€é€² Batch çš„å•†å“éƒ½æˆåŠŸç”¢å‡ºåœ–ç‰‡ã€‚")


def main():
    product_row_map, pre_skipped_rows, base_columns = build_jsonl_and_product_map(
        PRODUCT_FILE,
        JSONL_FILE,
    )
    run_batch_and_save_images(
        JSONL_FILE,
        OUTPUT_DIR,
        product_row_map,
        pre_skipped_rows,
        base_columns,
    )

if __name__ == "__main__":
    main()
